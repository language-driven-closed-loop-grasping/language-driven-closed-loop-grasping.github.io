<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language-Driven Closed-Loop Grasping with Model-Predictive
    Trajectory Replanning</title>
  <link rel="icon" type="image/x-icon" href="static/pdfs/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Driven Closed-Loop Grasping with Model-Predictive
              Trajectory Replanning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=T_LryjgAAAAJ&hl=en" target="_blank">Huy Hoang Nguyen</a><sup>1*</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <span class="author-block">
                  <a href="https://scholar.google.de/citations?user=H0GgsQ4AAAAJ&hl=de" target="_blank">Florian Beck</a><sup>2*</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=qyExc4QAAAAJ&hl=en" target="_blank">Minh Nhat Vu</a><sup>2,4**</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=PH_GwWwAAAAJ&hl=de" target="_blank">Gerald Ebmer</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  <span class="author-block">
                    <a href="https://www.csc.liv.ac.uk/~anguyen/" target="_blank">Anh Nguyen</a><sup>3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=ee4tJYgAAAAJ&hl=en" target="_blank">Andreas Kugi</a><sup>2,4</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Eötvös Loránd University</span>&nbsp;&nbsp;
                    <span class="author-block"><sup>2</sup>ACIN - TU Wien</span>&nbsp;&nbsp;
                    <span class="author-block"><sup>3</sup>University of Liverpool</span>&nbsp;&nbsp;
                    <span class="author-block"><sup>4</sup>Austrian Institute of Technology</span>&nbsp;&nbsp;
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>&nbsp;&nbsp;
                    <span class="eql-cntrb"><small><br><sup>**</sup>Corresponding Author</small></span>
                  </div>
                  <div class="logo-container">
                    <img
                      src="./static/images/elte_angol_fekvo_kek_logo.jpg" />
                    <img src="./static/images/acin-tuw.png" />
                    <img src="./static/images/images.png" />
                    <img
                      src="./static/images/ait_logo_ohne_claim_c1_rgb.jpg" />
                  </div>
                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://youtu.be/PAODfJkVXJo" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Combining a vision module inside a closed-loop control system for a seamless movement of a robot in a manipulation task is challenging due to the inconsistent update rates between utilized modules. This is even more difficult in a dynamic environment, e.g., objects are moving. 
            This paper presents a modular zero-shot framework for language-driven manipulation of (dynamic) objects through a closed-loop control system with real-time trajectory replanning and an online 6D object pose localization. 
            By leveraging a vision language model via natural language commands, an object is segmented within 0.5s. 
            Then, guided by natural language commands, a closed-loop system, including a unified pose estimation and tracking and online trajectory planning, is utilized to continuously track this object and compute the optimal trajectory in real time. This provides a smooth trajectory that avoids jerky movements and ensures the robot can grasp a non-stationary object. Experiment results exhibit the real-time capability of the proposed zero-shot modular framework for the trajectory optimization module to accurately and efficiently grasp moving objects, i.e., up to 30Hz update rates for the online 6D pose localization module and 10Hz update rates for the receding-horizon trajectory optimization. This highlights the modular framework's potential applications in robotics and human-robot interaction.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper video. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-two-thirds">
    <h2 class="title is-3">Video</h2>
    <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/PAODfJkVXJo?modestbranding=1&autohide=1&showinfo=0&controls=1"
        frameborder="0" allow="encrypted-media" allowfullscreen></iframe>
    </div>
  </div>
</div>
</section>
<!-- End teaser video -->

<!-- Method -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Method</h2>

      <div class="columns is-vcentered  is-centered">
        <img src="./static/images/flowchart.png" alt="method" />
        </br>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
        Our method involves the three main modules: language-driven object detection, real-time object pose localization, and online trajectory optimization (MP-TrajOpt).
        Consider a scenario in a workshop where a human instructs a robot to grasp a tool with a known CAD model and place it in a predefined location. First, the language-driven object detection module processes the image to determine the 2D location of the tool and generate a binary mask.
         This mask, combined with the CAD model, is used by the real-time object pose localization module to estimate the initial pose of the tool. The pose is then refined using a linear Kalman filter for smoothness.
          Finally, the online trajectory optimization module plans and executes the grasp and placement actions, ensuring smooth and collision-free movements. 
      </h2>
        
      </div>
    </div>
  </section>
<!--End Method -->

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiment</h2>
      <div id="results-grid" class="columns is-multiline">
        <div class="column is-one-third">
          <video poster="" id="metal" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_1.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the metal part"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="orange-block" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_2.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the orange block"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="plier" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_3.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the plier"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="scissor" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_4.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the scissor"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="stripper" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_5.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the line stripper"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="timer" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_6.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the timer"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="eraser" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_7.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the eraser"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="black-tool" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_8.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the black tool"</p>
        </div>
        <div class="column is-one-third">
          <video poster="" id="drill" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_9.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the drill"</p>
        </div>
        <div class="column is-one-third is-offset-one-third">
          <video poster="" id="silver-block" autoplay controls muted loop playsinline>
            <source src="./static/videos/video-RAL-submission_10.mp4" type="video/mp4">
          </video>
          <p class="title is-5">Prompt:"Grasp the silver block"</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

      <!-- Animation. -->
      <div class="rows is-centered ">
        <div class="row is-full-width">

            <!--/ Results. -->
            <h2 class="title is-3">Results</h2>

            <div class="container">
              <div class="columns is-vcentered  is-centered">
                <img src="./static/images/result.png" alt="results" />
                </br>
              </div>
              <br>
              <h2 class="subtitle has-text-centered">
                Results from our method show that the 3D position trajectory of the object closely matches the ground truth measurements(Opitrack). It is important to note that these measurements are transformed to the camera origin, which is approximately
                1 m away from the object. The maximum positional error observed is approximately 0.02 m, representing 2% relative
                to the distance to the camera. The orientation error is constrained, with a maximum error of 0.15 rad in the roll angle.
              </h2>
            </div>

            <br>
            <br>

</section>
<!-- End Result -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<section class="section" id="acknowledgements">
  <div class="container content is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p>We borrow the page template from  <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. Special thanks to them!
      <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
